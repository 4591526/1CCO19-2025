{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPaKy+XCX4vofttcBYBg+J+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Web Scraping**\n","\n","El web scraping es un método automático para obtener grandes cantidades de datos de sitios web. La mayoría de estos datos son datos no estructurados en formato HTML que luego se convierten en datos estructurados en una base de datos para que puedan usarse en diversas aplicaciones o en corpus de texto. Existen muchas maneras de realizar web scraping para obtener datos de sitios web. Estas incluyen el uso de servicios en línea, API específicas o incluso la creación de código para web scraping desde cero. Muchos sitios web grandes, como Google, Twitter, Facebook, StackOverflow, etc., tienen API que permiten acceder a sus datos en formato estructurado.\n","\n","Python es el lenguaje más popular para el web scraping, ya que gestiona la mayoría de los procesos con facilidad. Además, cuenta con diversas librerías creadas específicamente para el web scraping. Beautiful soup es una librería de Python muy adecuada para el web scraping. Crea un árbol de análisis que permite extraer datos del HTML de un sitio web. Beautiful soup también ofrece múltiples funciones para la navegación, la búsqueda y la modificación de estos árboles de análisis.\n","\n","\n","## **HTML**\n","* HTML significa lenguaje de marcado de hipertexto\n","* HTML es el lenguaje de marcado estándar para crear páginas web.\n","* HTML describe la estructura de una página web\n","* HTML consta de una serie de elementos\n","\n","\n","## **Estructura HTML**\n","\n","* La `<!DOCTYPE html>` declaración define que este documento es un documento HTML5\n","* El `<html>` elemento es el elemento raíz de una página HTML.\n","* El `<head>` elemento contiene metainformación sobre la página HTML\n","* El `<title>` elemento especifica un título para la página HTML (que se muestra en la barra de título del navegador o en la pestaña de la página)\n","* El `<body>` elemento define el cuerpo del documento y es un contenedor para todo el contenido visible, como encabezados, párrafos, imágenes, hipervínculos, tablas, listas, etc.\n","* El `<h1>` elemento define un encabezado grande\n","* El `<p>` elemento define un párrafo\n","\n","<!DOCTYPE html>\n","<html>\n","<head>\n","<title>Page Title</title>\n","</head>\n","<body>\n","\n","<h1>This is a Heading</h1>\n","<p>This is a paragraph.</p>\n","\n","</body>\n","</html>"],"metadata":{"id":"RW2RN_ZWu2b2"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"jIpJ0j3G-lki","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749665263420,"user_tz":300,"elapsed":9071,"user":{"displayName":"Luisa Gomez Saltachin","userId":"01453055480593483426"}},"outputId":"16ae989f-06b5-4dc5-a044-f841fb5bd6b0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.0)\n"]}],"source":["!pip install requests beautifulsoup4\n","# Instalamos las librerías requests y beautifulsoup4 para poder hacer peticiones a páginas web y\n","# luego analizar su contenido HTML desde Google Colab."]},{"cell_type":"markdown","source":["* !: El signo de exclamación al inicio indica que se está ejecutando un comando del sistema operativo (como si lo escribieras en la terminal o consola), no una instrucción de Python.\n","\n","* pip install: pip es el sistema de gestión de paquetes de Python. install le dice a pip que debe instalar algo.\n","\n","* requests: Es una librería de Python para hacer peticiones HTTP de manera sencilla. Se usa mucho para descargar contenido de páginas web o consumir APIs.\n","\n","* beautifulsoup4: Es una librería para extraer y procesar datos de archivos HTML o XML. Muy útil para hacer scraping web (leer contenido estructurado de una página web)."],"metadata":{"id":"dApHm-OWMqkD"}},{"cell_type":"code","source":["import requests # Esto importa la librería requests, que se usa para hacer solicitudes HTTP (como acceder a una página web desde código).\n","from bs4 import BeautifulSoup # Esta línea importa la clase BeautifulSoup desde el módulo bs4. Se utiliza para analizar el HTML o XML descargado, y extraer de él elementos específicos, como títulos, enlaces, párrafos, etc."],"metadata":{"id":"3UANX2R_-tFc","executionInfo":{"status":"ok","timestamp":1749665306721,"user_tz":300,"elapsed":548,"user":{"displayName":"Luisa Gomez Saltachin","userId":"01453055480593483426"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Creamos una variable con el url de la noticia\n","url = 'https://rpp.pe/economia/economia/aeropuerto-jorge-chavez-ositran-multa-a-lap-con-mas-de-s-3-millones-por-instalar-vidrios-defectuosos-en-la-torre-de-control-noticia-1639875'"],"metadata":{"id":"XTkPAsg4OQkB","executionInfo":{"status":"ok","timestamp":1749665349803,"user_tz":300,"elapsed":35,"user":{"displayName":"Luisa Gomez Saltachin","userId":"01453055480593483426"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["| Código | Significado                | ¿Qué hacer?                                                   |\n","| ------ | -------------------------- | ------------------------------------------------------------- |\n","| `200`  | OK (todo bien)             | ✅ Puedes seguir                                               |\n","| `403`  | Prohibido                  | ❌ El servidor te bloqueó (a veces por protección contra bots) |\n","| `404`  | No encontrado              | ❌ La página no existe                                         |\n","| `500`  | Error interno del servidor | ❌ Problema del sitio web, no tuyo                             |\n"],"metadata":{"id":"iOpbMeoxO_BK"}},{"cell_type":"code","source":["# requests.get(url): realiza una petición HTTP de tipo GET a la dirección web que está en la variable\n","response = requests.get(url)\n","\n","# Comprobamos si la solicitud fue exitosa\n","# response.status_code: accede al código de estado HTTP que devuelve el servidor.\n","if response.status_code == 200:\n","    print(\"Página cargada correctamente\")\n","else:\n","    print(\"Error al acceder a la página:\", response.status_code)\n"],"metadata":{"id":"8YfkkmuA-tMM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749665423618,"user_tz":300,"elapsed":454,"user":{"displayName":"Luisa Gomez Saltachin","userId":"01453055480593483426"}},"outputId":"fda0e603-40f9-45db-cff4-fc0f27b76d2e"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Página cargada correctamente\n"]}]},{"cell_type":"code","source":["# Convierte el HTML descargado desde la web (a través de requests) en un objeto soup que puedes explorar fácilmente para encontrar etiquetas, atributos, texto, etc.\n","soup = BeautifulSoup(response.content, 'html.parser')\n"],"metadata":{"id":"vVZfPK_t-tO9","executionInfo":{"status":"ok","timestamp":1749665484842,"user_tz":300,"elapsed":86,"user":{"displayName":"Luisa Gomez Saltachin","userId":"01453055480593483426"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["* BeautifulSoup es una clase de la biblioteca bs4 (Beautiful Soup 4) que permite analizar documentos HTML o XML.\n","\n","* Crea un árbol de objetos a partir del contenido de una página web. Ese árbol permite buscar, navegar y extraer partes específicas como <title>, <p>, <a>, etc."],"metadata":{"id":"xBGY5lAMPl25"}},{"cell_type":"code","source":["# Buscamos todos los párrafos\n","# soup.find_all('p') busca todas las etiquetas <p> en el documento HTML.\n","parrafos = soup.find_all('p')\n","\n","# Limpiamos y unimos el texto\n","# Creamos una variable llamada texto_noticia como una cadena vacía ('').\n","texto_noticia = ''\n","\n","# Este es un bucle for que recorre cada elemento p dentro de la lista parrafos.\n","for p in parrafos: # p representa un párrafo\n","    texto = p.get_text() # p.get_text() extrae solo el texto limpio dentro de esa etiqueta, sin etiquetas HTML ni código.\n","    texto_noticia += texto + '\\n'  # Concatenamos ese texto a texto_noticia usando +=, y le añadimos un salto de línea '\\n' para separar cada párrafo y que el texto final sea legible.\n","\n","# Mostrar los primeros 1000 caracteres\n","print(texto_noticia[:1000])\n"],"metadata":{"id":"Fo_o0LS6-84r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749665561630,"user_tz":300,"elapsed":25,"user":{"displayName":"Luisa Gomez Saltachin","userId":"01453055480593483426"}},"outputId":"6d4b1088-aff9-45ce-d4de-99083df3729d"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["por Fiorella Hokama\n","Francisco Jaramillo, gerente de supervisión y supervisión del Organismo Supervisor de la Inversión en Infraestructura de Transporte de Uso Público (Ositrán), se presentó en Economía Para Todos por RPP y detalló las acciones que viene realizando la entidad.\n","\"En este terminal que está en operación, Ositrán viene realizando una supervisión y cumplimiento de niveles de servicio\", dijo al iniciar y agregó que \"si se determina un incumplimiento, se realizará un procedimiento administrativo sancionador\".\n","► ¿Tienes un viaje al exterior? Así funciona el nuevo sistema Migracheck en el Aeropuerto Jorge Chávez\n","El especialista confirmó que no haber entregado el Nuevo Terminal del Aeropuerto Jorge Chávez el 30 de enero, es una falta en el contrato con Lima Airport Partners (LAP), por lo cual \"esa es otra imputación por no haber concluido y puesto en operación en la fecha establecida. Esa es otra imputación que en su momento se hará de conocimiento que le aplica una sanción al res\n"]}]},{"cell_type":"code","source":["# Guardamos el contenido en un archivo de texto\n","with open('noticia.txt', 'w', encoding='utf-8') as archivo:\n","    archivo.write(texto_noticia)\n","\n","print(\"Contenido guardado en 'noticia.txt'\")"],"metadata":{"id":"tnrhdHq3_RVD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749665637342,"user_tz":300,"elapsed":11,"user":{"displayName":"Luisa Gomez Saltachin","userId":"01453055480593483426"}},"outputId":"afcb2a83-7e8e-4c48-8d94-1f27680d9dc2"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Contenido guardado en 'noticia.txt'\n"]}]},{"cell_type":"markdown","source":["* open() es una función para abrir un archivo. Aquí la usamos para crear o sobrescribir un archivo llamado 'noticia.txt'.\n","\n","* El modo 'w' significa escritura (write): si el archivo ya existe, se borra el contenido anterior y se escribe de nuevo.\n","\n","* encoding='utf-8' asegura que el archivo se guarde con codificación UTF-8, que soporta caracteres especiales (acentos, ñ, etc.), muy común en textos en español.\n","\n","* with es una estructura que abre el archivo y lo cierra automáticamente cuando termina el bloque, evitando errores y liberando recursos.\n","\n","* archivo es el nombre que damos al objeto que representa el archivo abierto."],"metadata":{"id":"vH67RO0CQ30j"}},{"cell_type":"code","source":["# Lista con las URLs que quieres procesar\n","urls = [\n","    'https://rpp.pe/politica/actualidad/yonhy-lescano-acuso-al-gobierno-de-coordinar-la-fuga-de-nadine-heredia-a-brasil-noticia-1629266',\n","    'https://elcomercio.pe/politica/justicia/ollanta-humala-y-nadine-heredia-fiscalia-presenta-126-testigos-y-casi-700-pruebas-documentales-en-acusacion-por-caso-gasoducto-sur-noticia/'\n","]\n","\n","for i, url in enumerate(urls, start=1):\n","    # Enumeramos las URLs para tener un contador 'i' empezando en 1\n","    # 'i' es el número de la noticia, 'url' es la dirección web a procesar\n","\n","    print(f\"\\nProcesando noticia {i}:\\n{url}\")\n","    # Imprimimos un mensaje indicando qué noticia se está procesando y su URL\n","\n","    # Solicitar la página web con la URL actual\n","    response = requests.get(url)\n","\n","    # Verificar si la solicitud fue exitosa (código 200 significa éxito)\n","    if response.status_code != 200:\n","        # Si el código no es 200, imprimir un mensaje de error con el código recibido\n","        print(f\"Error al acceder a la página {url}, código: {response.status_code}\")\n","\n","        # Saltar a la siguiente iteración del ciclo (no procesar esta URL)\n","        continue\n","\n","    # Parseamos el contenido HTML de la página usando BeautifulSoup con el parser 'html.parser'\n","    soup = BeautifulSoup(response.content, 'html.parser')\n","\n","    # Extraemos todos los párrafos <p>\n","    parrafos = soup.find_all('p')\n","\n","    # Extraemos y unimos el texto\n","    texto_noticia = ''\n","    for p in parrafos:\n","        texto_noticia += p.get_text() + '\\n'\n","\n","    # Guardamos cada noticia en un archivo separado\n","    nombre_archivo = f'noticia_{i}.txt'\n","    with open(nombre_archivo, 'w', encoding='utf-8') as archivo:\n","        archivo.write(texto_noticia)\n","\n","    print(f\"Noticia {i} guardada en '{nombre_archivo}'\")"],"metadata":{"id":"2p-2IZiARVX-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749665725415,"user_tz":300,"elapsed":610,"user":{"displayName":"Luisa Gomez Saltachin","userId":"01453055480593483426"}},"outputId":"877d397c-7e87-4ed7-bfcb-bed53f15b135"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Procesando noticia 1:\n","https://rpp.pe/politica/actualidad/yonhy-lescano-acuso-al-gobierno-de-coordinar-la-fuga-de-nadine-heredia-a-brasil-noticia-1629266\n","Noticia 1 guardada en 'noticia_1.txt'\n","\n","Procesando noticia 2:\n","https://elcomercio.pe/politica/justicia/ollanta-humala-y-nadine-heredia-fiscalia-presenta-126-testigos-y-casi-700-pruebas-documentales-en-acusacion-por-caso-gasoducto-sur-noticia/\n","Noticia 2 guardada en 'noticia_2.txt'\n"]}]},{"cell_type":"code","source":["# Lista con las URLs que quieres procesar\n","urls = [\n","    'https://rpp.pe/politica/actualidad/yonhy-lescano-acuso-al-gobierno-de-coordinar-la-fuga-de-nadine-heredia-a-brasil-noticia-1629266',\n","    'https://elcomercio.pe/politica/justicia/ollanta-humala-y-nadine-heredia-fiscalia-presenta-126-testigos-y-casi-700-pruebas-documentales-en-acusacion-por-caso-gasoducto-sur-noticia/'\n","]\n","\n","# Creamos el archivo 'noticias_combinadas.txt' en modo escritura ('w') con codificación UTF-8\n","# Esto permite escribir texto en el archivo y asegura que caracteres especiales se guarden bien\n","with open('noticias_combinadas.txt', 'w', encoding='utf-8') as archivo:\n","\n","    # Recorremos la lista de URLs, enumerándolas desde 1 para tener un contador de noticias\n","    for i, url in enumerate(urls, start=1):\n","        # Mostramos en consola cuál noticia estamos procesando y la URL\n","        print(f\"\\nProcesando noticia {i}:\\n{url}\")\n","\n","        # Realizamos la solicitud HTTP para obtener el contenido de la página web\n","        response = requests.get(url)\n","\n","        # Verificamos que la solicitud haya sido exitosa (código 200)\n","        if response.status_code != 200:\n","            # Si hay error, mostramos un mensaje con el código de error y pasamos a la siguiente URL\n","            print(f\"Error al acceder a la página {url}, código: {response.status_code}\")\n","            continue\n","\n","        # Parseamos el contenido HTML de la página usando BeautifulSoup con el parser 'html.parser'\n","        soup = BeautifulSoup(response.content, 'html.parser')\n","\n","        # Buscamos todos los elementos <p> que normalmente contienen párrafos de texto\n","        parrafos = soup.find_all('p')\n","\n","        # Inicializamos una cadena vacía para acumular el texto de los párrafos\n","        texto_noticia = ''\n","\n","        # Recorremos cada párrafo encontrado\n","        for p in parrafos:\n","            # Extraemos solo el texto limpio del párrafo y lo agregamos a la variable, con un salto de línea\n","            texto_noticia += p.get_text() + '\\n'\n","\n","        # Escribimos en el archivo un separador para distinguir las noticias\n","        archivo.write(f\"\\n--- Noticia {i} ---\\n\\n\")\n","\n","        # Escribimos el texto completo de la noticia en el archivo\n","        archivo.write(texto_noticia)\n","\n","        # Informamos que la noticia fue agregada correctamente al archivo\n","        print(f\"Noticia {i} agregada a 'noticias_combinadas.txt'\")\n","\n","# Mensaje final para indicar que todas las noticias fueron procesadas y guardadas\n","print(\"¡Todas las noticias guardadas en un solo archivo!\")"],"metadata":{"id":"ICB3FfEHRiJf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749665825254,"user_tz":300,"elapsed":1296,"user":{"displayName":"Luisa Gomez Saltachin","userId":"01453055480593483426"}},"outputId":"b9753011-eff8-4ff9-e671-8f8b16c532a3"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Procesando noticia 1:\n","https://rpp.pe/politica/actualidad/yonhy-lescano-acuso-al-gobierno-de-coordinar-la-fuga-de-nadine-heredia-a-brasil-noticia-1629266\n","Noticia 1 agregada a 'noticias_combinadas.txt'\n","\n","Procesando noticia 2:\n","https://elcomercio.pe/politica/justicia/ollanta-humala-y-nadine-heredia-fiscalia-presenta-126-testigos-y-casi-700-pruebas-documentales-en-acusacion-por-caso-gasoducto-sur-noticia/\n","Noticia 2 agregada a 'noticias_combinadas.txt'\n","¡Todas las noticias guardadas en un solo archivo!\n"]}]}]}